{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is a parameter?\n",
    "A parameter is a value that is used to control or influence the behavior of a system, function, or process. The meaning of \"parameter\" depends on the context:\n",
    "# In Mathmatics\n",
    "# In Programming\n",
    "# In Science and Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What is correlation?What does negative correlation mean?\n",
    "Correlation is a statistical measure that describes the relationship between two variablesâ€”how they move together. It ranges from -1 to +1:\n",
    "\n",
    "Positive Correlation (+1 to 0): When one variable increases, the other tends to increase too (e.g., height and weight).\n",
    "Negative Correlation (0 to -1): When one variable increases, the other tends to decrease (e.g., the more time spent exercising, the lower the body fat percentage).\n",
    "Zero Correlation (0): No relationship between the variables.\n",
    "\n",
    "# A negative correlation means that as one variable goes up, the other goes down. For example, if the price of a product increases and its sales decrease, they have a negative correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define Machine Learning. What are the main components in Machine Learning?\n",
    "Machine Learning is a branch of artificial intelligence (AI) that enables computers to learn patterns from data and make predictions or decisions without being explicitly programmed. It involves training models using data and improving their performance over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How does loss value help in determining whether the model is good or not?\n",
    "The loss value helps determine how well a machine learning model is performing by measuring the difference between the modelâ€™s predictions and the actual target values. It serves as a key indicator of model accuracy and effectiveness.\n",
    "\n",
    "How Loss Value Helps in Model Evaluation\n",
    "\n",
    "Lower Loss = Better Model\n",
    "A low loss value means the modelâ€™s predictions are close to the actual values, indicating good performance.\n",
    "A high loss value means the model is making large errors, suggesting poor performance.\n",
    "\n",
    "Loss Function Choice Matters\n",
    "Different ML problems use different loss functions (e.g., Mean Squared Error for regression, Cross-Entropy for classification).\n",
    "The right choice ensures the model learns effectively.\n",
    "\n",
    "Training vs. Validation Loss\n",
    "If both training and validation loss are low, the model is performing well.\n",
    "If training loss is low but validation loss is high, the model is overfitting (memorizing training data instead of generalizing).\n",
    "If both training and validation loss are high, the model is underfitting (not learning enough patterns)\n",
    "\n",
    "Tracking Loss for Model Improvement\n",
    "Loss is minimized using optimization techniques like gradient descent.\n",
    "Loss trends over epochs (iterations) show if the model is improving or if adjustments are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. What are continuous and categorical variables?\n",
    "Continuous Variables\n",
    "A continuous variable is a numeric variable that can take an infinite number of values within a given range. These values can be measured and often include decimals or fractions.\n",
    "\n",
    "Examples:\n",
    "Height (e.g., 5.8 ft, 6.2 ft)\n",
    "\n",
    "Categorical Variables\n",
    "A categorical variable represents distinct groups or categories. These values do not have a numerical meaning and cannot be measured on a continuous scale.\n",
    "\n",
    "Types of Categorical Variables:\n",
    "Nominal: No inherent order (e.g., colors: Red, Blue, Green; Gender: Male, Female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "Since most machine learning models work with numerical data, categorical variables need to be converted into numerical format before training. Below are common techniques used:\n",
    " \n",
    " One-Hot Encoding (OHE)\n",
    "Converts each category into a binary (0/1) vector.\n",
    "Works well for nominal variables (unordered categories).\n",
    "Can lead to high-dimensionality if there are too many unique categories.\n",
    "\n",
    "Label Encoding\n",
    "Assigns a unique integer to each category.\n",
    "Works well for ordinal categorical variables (where order matters).\n",
    "Can introduce misleading numerical relationships for nominal categories.\n",
    "\n",
    "Ordinal Encoding\n",
    "Similar to label encoding but used specifically for ordinal data where category order matters.\n",
    "\n",
    "Frequency Encoding\n",
    "Replaces each category with the frequency of its occurrence in the dataset.\n",
    "Helps capture categorical importance but can still introduce some bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. What do you mean by training and testing a dataset?\n",
    "In Machine Learning, a dataset is typically split into two parts: training data and testing data to evaluate how well a model performs on unseen data.\n",
    "\n",
    "1. Training Dataset\n",
    "The training dataset is the portion of the data used to train the model.\n",
    "The model learns patterns, relationships, and features from this data.\n",
    "The process involves adjusting the modelâ€™s parameters using techniques like gradient descent to minimize errors.\n",
    "ðŸ”¹ Example:If youâ€™re building a spam email classifier, the training dataset will contain labeled emails (Spam or Not Spam) that the model uses to learn.\n",
    "\n",
    "2. Testing Dataset\n",
    "The testing dataset is the portion of the data used to evaluate the model's performance after training.\n",
    "This helps check if the model can generalize to new, unseen data.\n",
    "The model does not learn from the test dataâ€”it is only used for validation.\n",
    "ðŸ”¹ Example:After training the spam classifier, we test it on new emails (not seen during training) to check how well it predicts spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. What is sklearn.preprocessing?\n",
    "sklearn.preprocessing is a module in Scikit-Learn that provides tools for transforming raw data into a suitable format for machine learning models. It helps improve model performance by scaling, normalizing, encoding, and transforming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. What is a Test set?\n",
    "A test set is a portion of a dataset that is used to evaluate the performance of a trained machine learning model. It helps determine how well the model can generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.  How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
    "\n",
    "1. Splitting Data for Model Fitting (Training & Testing) in Python\n",
    "Step 1: Define the Problem\n",
    "Step 2: Collect & Explore Data\n",
    "Step 3: Preprocess the Data\n",
    "Step 4: Split Data into Training & Testing Sets\n",
    "Step 5: Choose a Model\n",
    "Step 6: Train the Model\n",
    "Step 7: Evaluate the Model\n",
    "Step 8: Tune Hyperparameters\n",
    "Step 9: Deploy the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Study_Hours  Exam_Score\n",
      "count     10.00000   10.000000\n",
      "mean       5.50000   72.500000\n",
      "std        3.02765   15.138252\n",
      "min        1.00000   50.000000\n",
      "25%        3.25000   61.250000\n",
      "50%        5.50000   72.500000\n",
      "75%        7.75000   83.750000\n",
      "max       10.00000   95.000000\n",
      "Mean Squared Error: 0.0\n",
      "Predicted Exam Score for 7 hours of study: 80.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hari Vignesh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 10\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 1: Load and Explore Data\n",
    "data = {'Study_Hours': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'Exam_Score': [50, 55, 60, 65, 70, 75, 80, 85, 90, 95]}\n",
    "df = pd.DataFrame(data)\n",
    "print(df.describe())  # Check basic statistics\n",
    "\n",
    "# Step 2: Split Data into Training and Testing Sets\n",
    "X = df[['Study_Hours']]  # Features\n",
    "y = df['Exam_Score']  # Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Data Preprocessing (Scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train the Model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Make Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Step 7: Predict for a New Value\n",
    "new_data = np.array([[7]])  # Example: 7 hours of study\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "predicted_score = model.predict(new_data_scaled)\n",
    "print(f\"Predicted Exam Score for 7 hours of study: {predicted_score[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Why do we have to perform EDA before fitting a model to the data?\n",
    "Exploratory Data Analysis (EDA) is a crucial step in machine learning that helps understand the dataset before applying any model. Skipping EDA can lead to poor model performance, biases, and misleading results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. What is correlation?\n",
    "Correlation measures the relationship between two variables and indicates how one variable changes in relation to another. It helps identify patterns in data and is crucial in feature selection for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. What does negative correlation mean?\n",
    "Negative correlation means that when one variable increases, the other variable decreases (and vice versa). It indicates an inverse relationship between two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. How can you find correlation between variables in Python?\n",
    "In Python, we can compute the correlation between variables using Pandas and Seaborn. The most common method is Pearsonâ€™s correlation coefficient, but there are also other methods like Spearman and Kendall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Study_Hours  Exam_Score  TV_Hours\n",
      "Study_Hours      1.00000     0.99544  -1.00000\n",
      "Exam_Score       0.99544     1.00000  -0.99544\n",
      "TV_Hours        -1.00000    -0.99544   1.00000\n"
     ]
    }
   ],
   "source": [
    "#14 \n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Study_Hours': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'Exam_Score': [50, 55, 60, 70, 75, 80, 85, 90],\n",
    "    'TV_Hours': [8, 7, 6, 5, 4, 3, 2, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. What is causation? Explain difference between correlation and causation with an example.\n",
    "What is Causation?\n",
    "Causation means that one event directly causes another event to happen. If variable A changes and this directly leads to a change in variable B, we say there is a cause-and-effect relationship between them.\n",
    "For example, if you increase the number of hours you study, your exam score improves. Here, studying more directly causes better performance.\n",
    "\n",
    "What is Correlation?\n",
    "Correlation means that two variables are related, but one does not necessarily cause the other to change. They may move together due to coincidence or because of another hidden factor (confounding variable).\n",
    "For example, ice cream sales and drowning incidents both increase in the summer. This does not mean that eating ice cream causes drowning. Instead, hot weather is the real reason why both happen at the same time.\n",
    "\n",
    "Key Differences Between Correlation and Causation\n",
    "Correlation shows that two variables change together but does not prove one causes the other.\n",
    "Causation means that a change in one variable directly leads to a change in another.\n",
    "Correlation can be due to a third hidden factor (confounder), while causation requires solid proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "Types of Optimizers in Machine Learning\n",
    "1. Gradient Descent (GD)\n",
    "Gradient Descent is the most basic optimization algorithm. It updates model parameters by moving in the direction of the negative gradient of the loss function.\n",
    "\n",
    "Types of Gradient Descent:\n",
    "Batch Gradient Descent (BGD) â€“ Updates weights after computing the gradient on the entire dataset.\n",
    "Stochastic Gradient Descent (SGD) â€“ Updates weights after each training example, making it faster but noisier.\n",
    "Mini-Batch Gradient Descent â€“ Updates weights after a small batch of training samples, balancing speed and stability.\n",
    "\n",
    "Example of SGD in Python:\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "\n",
    "2. Momentum Optimizer\n",
    "Momentum adds a velocity term to the gradient, helping the model accelerate in the right direction and avoid getting stuck in local minima.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "âœ” Helps with faster convergence and reduces oscillations.\n",
    "\n",
    "3. Adagrad (Adaptive Gradient Algorithm)\n",
    "Adagrad adapts the learning rate for each parameter, making frequent updates smaller and rare updates larger. It is useful for sparse data.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "\n",
    "optimizer = Adagrad(learning_rate=0.01)\n",
    "âœ” Works well in text processing (NLP) but can slow down over time.\n",
    "\n",
    "4. RMSprop (Root Mean Square Propagation)\n",
    "RMSprop adjusts the learning rate using a moving average of past squared gradients, preventing it from becoming too small.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "âœ” Works well for recurrent neural networks (RNNs) and prevents the learning rate from decaying too much.\n",
    "\n",
    "5. Adam (Adaptive Moment Estimation)\n",
    "Adam combines Momentum and RMSprop, adjusting the learning rate for each parameter based on past gradients.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "âœ” Most popular optimizer for deep learning due to its fast convergence and stability.\n",
    "\n",
    "6. AdamW (Adam with Weight Decay)\n",
    "AdamW is a modified version of Adam that includes weight decay, preventing overfitting.\n",
    "\n",
    "Example:\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "optimizer = AdamW(learning_rate=0.001, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. What is sklearn.linear_model ?\n",
    "sklearn.linear_model is a module in Scikit-Learn that provides various linear models for regression and classification tasks. These models assume a linear relationship between input features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. What does model.fit() do? What arguments must be given?\n",
    "What does model.fit() do?\n",
    "The .fit() method trains a machine learning model by adjusting its internal parameters (weights & biases) using the provided training data. It learns patterns from input features (X) and their corresponding target values (y) to make predictions on new data.\n",
    "\n",
    "Arguments for model.fit()\n",
    "The required arguments depend on the type of model used. In general:\n",
    "\n",
    "1. For Supervised Learning (Regression & Classification)\n",
    "model.fit(X, y)\n",
    "X â†’ Feature matrix (independent variables)\n",
    "\n",
    "y â†’ Target variable (dependent variable)\n",
    "\n",
    "âœ… Used in models like Linear Regression, Logistic Regression, Decision Trees, etc.\n",
    "\n",
    "Example (Linear Regression):\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = [[1], [2], [3], [4], [5]]  # Features\n",
    "y = [10, 20, 30, 40, 50]       # Target\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)  # Model learns the relationship between X and y\n",
    "\n",
    "2. For Unsupervised Learning (Clustering, Dimensionality Reduction)\n",
    "model.fit(X)\n",
    "X â†’ Only features are needed (no y) since there are no predefined labels.\n",
    "âœ… Used in models like K-Means, PCA, DBSCAN, etc.\n",
    "\n",
    "Example (K-Means Clustering):\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8]]  # Only features\n",
    "\n",
    "model = KMeans(n_clusters=2)\n",
    "model.fit(X)  # Finds cluster centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 What does model.predict() do? What arguments must be given?\n",
    "What does model.predict() do?\n",
    "The .predict() method makes predictions using a trained machine learning model. After the model has learned patterns from training data using .fit(), .predict() is used to estimate outputs for new, unseen data.\n",
    "\n",
    "Arguments for model.predict()\n",
    "The required argument is:\n",
    "X_new â†’ New\n",
    "Must have the same number of features as the training data.\n",
    "Shape: (num_samples, num_features).\n",
    "predictions = model.predict(X_new)\n",
    "Example 1: Predicting with Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60. 70. 80.]\n"
     ]
    }
   ],
   "source": [
    "# 19\n",
    "#Training data\n",
    "X_train = [[1], [2], [3], [4], [5]]\n",
    "y_train = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict for new values\n",
    "X_new = [[6], [7], [8]]  # New feature values\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "print(predictions)  # Output: [60, 70, 80] (based on learned relationship)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20.  What are continuous and categorical variables?\n",
    "Continuous Variables\n",
    "ðŸ”¹ Definition: Continuous variables are numerical values that can take any value within a range. They can be measured and have decimal points.\n",
    "ðŸ”¹ Examples:\n",
    "Height (e.g., 170.5 cm)\n",
    "\n",
    " Categorical Variables\n",
    "ðŸ”¹ Definition: Categorical variables represent groups or categories. They do not have a numerical meaning but rather label different groups.\n",
    "ðŸ”¹ Types of Categorical Variables:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. What is feature scaling? How does it help in Machine Learning?\n",
    "What is Feature Scaling?\n",
    "Feature scaling is the process of normalizing or standardizing numerical features so that they have the same scale. This prevents certain features from dominating others due to their larger range of values.\n",
    "Why is Feature Scaling Important?\n",
    "âœ” Prevents Bias in Models â†’ Large values can dominate smaller ones.\n",
    "âœ” Speeds Up Training â†’ Helps gradient descent converge faster.\n",
    "âœ” Improves Model Accuracy â†’ Works better with distance-based algorithms.\n",
    "âœ” Required for Some ML Algorithms â†’ Especially for models that depend on magnitude differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. How do we perform scaling in Python?\n",
    "Feature scaling can be done using scikit-learnâ€™s preprocessing module. Below are the three main techniques:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  ]\n",
      " [0.25]\n",
      " [0.5 ]\n",
      " [0.75]\n",
      " [1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# 22\n",
    "#  Min-Max Scaling (Normalization)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[100], [200], [300], [400], [500]])\n",
    "\n",
    "# Apply Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)  # Values scaled between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.41421356]\n",
      " [-0.70710678]\n",
      " [ 0.        ]\n",
      " [ 0.70710678]\n",
      " [ 1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "#22\n",
    "# Standardization (Z-Score Scaling)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)  # Mean = 0, Standard Deviation = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. ]\n",
      " [-0.5]\n",
      " [ 0. ]\n",
      " [ 0.5]\n",
      " [ 1. ]]\n"
     ]
    }
   ],
   "source": [
    "#22\n",
    "#Robust Scaling (Resistant to Outliers)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Apply Robust Scaling\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)  # Less affected by outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. What is sklearn.preprocessing?\n",
    "sklearn.preprocessing in Machine Learning\n",
    "The sklearn.preprocessing module in scikit-learn provides various functions to transform raw data into a suitable format for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24. How do we split data for model fitting (training and testing) in Python?\n",
    "In Machine Learning, we split data into training and testing sets to evaluate model performance.\n",
    "Using train_test_split() in Scikit-Learn\n",
    "The train_test_split() function from sklearn.model_selection is commonly used to randomly split data into training and testing sets.\n",
    "train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: [ 6  1  8  3 10  5  4  7]\n",
      "X_test: [9 2]\n",
      "y_train: [1 0 1 0 1 0 1 0]\n",
      "y_test: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# 24\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Sample Data (X: Features, y: Target)\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])  # Features\n",
    "y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])  # Target labels\n",
    "\n",
    "# Split data (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display Results\n",
    "print(\"X_train:\", X_train.ravel())  \n",
    "print(\"X_test:\", X_test.ravel())    \n",
    "print(\"y_train:\", y_train)  \n",
    "print(\"y_test:\", y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. Explain data encoding\n",
    "Data encoding is the process of converting categorical variables into numerical values so that machine learning models can process them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
